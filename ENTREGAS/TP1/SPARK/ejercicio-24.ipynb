{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["fRXO2ZDNa3c1"],"authorship_tag":"ABX9TyPDDJ7vihGUozGYdXrWM3aI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["24) Dado un tamaño de vocabulario parametrizable y una lista de stopwords también parametrizable implemente tf-IDF para los textos de los contenidos de forma distribuida. Debe obtener un vector por cada texto (⭐⭐⭐)."],"metadata":{"id":"JwasuLZrnGLq"}},{"cell_type":"markdown","source":["**Analisis previo**\n","\n","1) La respuesta al ejercicio sera un RDD cuyos valores definen unívocamente la matriz de TF-IDF (y por lo tanto definen el vector para cada texto).\n","\n","2) Una palabra debe tener 2 caracteres como minimo para ser considerada.\n","\n","3)Tenemos valores None en la columna \"text\" la cual nos interesa, entonces antes de trabajar eliminamos estos datos."],"metadata":{"id":"zrYonsuVjxvH"}},{"cell_type":"code","source":["!pip install pyspark\n","!pip install -U -q PyDrive\n","!apt update\n","!apt install openjdk-8-jdk-headless -qq\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext\n","from pyspark.sql import SQLContext\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","import math\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')"],"metadata":{"id":"TIsc2EGLnN9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["featuresParametrizable = 5\n","\n","stopwordsParametrizable = set(stopwords.words('spanish')).union(stopwords.words('english'))"],"metadata":{"id":"6EJU3JmKQoRT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Resolucion explicativa**"],"metadata":{"id":"fRXO2ZDNa3c1"}},{"cell_type":"code","source":["spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","sqlContext = SQLContext(sc)\n","df = sqlContext.read.csv('/content/drive/MyDrive/Organizacion de Datos/Colab Notebooks/TP1/contents_text_sample.csv', header=True, inferSchema=True, escape='\"', multiLine=True)\n","rdd = df.rdd"],"metadata":{"id":"-teLgCqatJ2L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683581718902,"user_tz":180,"elapsed":46989,"user":{"displayName":"Alan Valdevenito","userId":"11872677415514201254"}},"outputId":"94d87316-9292-436a-d749-364828c43675"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Pasos\n","\n","1) Con la transformacion map() buscamos obtener registros de la forma (id de texto, lista de palabras del texto) eliminando los caracteres que no nos interesan.\n","\n","2) Con la transformacion map() buscamos obtener registros de la forma (id de texto, lista de palabras del texto) donde además eliminamos aquellas palabras que no cumplen con tener 2 caracteres como minimo y aquellas palabras que son stopwords."],"metadata":{"id":"NglLa93ja9rm"}},{"cell_type":"code","source":["rdd_map = rdd.filter(lambda x: x.text != None).map(lambda x: (x.id, re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", x.text.lower())))\\\n","             .map(lambda x: (x[0], [palabra for palabra in x[1] if ((len(palabra) > 1) and (palabra not in stopwordsParametrizable))])).cache()\n","\n","cantidad_documentos = rdd_map.count()"],"metadata":{"id":"OnGHvpDHL778"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pasos: Hallamos las K palabras mas comunes. Notemos que K es el tamaño de vocabulario parametrizable que se menciona en el enunciado.\n","\n","4) Con la transformacion flatMap() buscamos unir todas las palabras en un unico registro.\n","\n","5) Con la transformacion map() buscamos obtener registros de la forma (palabra, 1) para poder contar la frecuencia de dichas palabras.\n","\n","6) Con la transformacion reduceByKey() buscamos sumar los registros para una misma clave.\n","\n","7) Con la accion takeOrdered() buscamos hallar las primeras K palabras de mayor a menor. Es decir, las K mas frecuentes."],"metadata":{"id":"0cDzEBBLb3cm"}},{"cell_type":"code","source":["palabras_comunes = rdd_map.flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).takeOrdered(featuresParametrizable, lambda x: -x[1])\n","palabras_comunes = [x[0] for x in palabras_comunes]"],"metadata":{"id":"zXLbye94S1cK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pasos: Contamos la frecuencia de las K palabras mas comunes en cada documento.\n","\n","8) Con la transformacion flatMap() buscamos obtener registros de la forma ((id de texto, palabra), 1) si la palabra es una de las mas comunes. Utilizamos un ciclo for ya que buscamos recorrer un registro para separarlo en varios.\n","\n","9) Con la transformacion reduceByKey() buscamos sumar los registros para una misma clave. Es decir, buscamos obtener la frecuencia de cada palabra segun el texto. Notemos que en este RDD nos faltan aquellas palabras comunes que tienen frecuencia 0."],"metadata":{"id":"ghTeHQRfctub"}},{"cell_type":"code","source":["rdd_flatmap = rdd_map.flatMap(lambda x: [((x[0], palabra), 1) for palabra in x[1] if palabra in palabras_comunes]).cache()\n","rdd_frecuencias_por_texto = rdd_flatmap.reduceByKey(lambda x,y: x+y)"],"metadata":{"id":"1Z3XP4c-Sw8j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pasos: Construimos un vector para cada documento de la forma [(Texto, Palabra), Frecuencia].\n","\n","10) Con la transformacion flatMap() buscamos obtener registros de la forma ((id de texto, palabra), 0) para cada una de las palabras mas comunes. Notemos que  en este RDD nos faltan las frecuencias de cada palabra segun el texto.\n","\n","11) Con la transformacion rightOuterJoin() buscamos obtener un RDD de la forma ((Texto, Palabra), (Frecuencia, 0)) donde si tenemos un valor distinto de None en Frecuencia entonces quiere decir que dicha palabra esta al menos 1 vez en el texto y nos quedamos con este valor, en caso contrario la palabra no aparece en el texto y nos quedamos con frecuencia igual a 0."],"metadata":{"id":"FxQ1b1o9dq9b"}},{"cell_type":"code","source":["rdd_vectores = rdd_map.flatMap(lambda x: [((x[0], palabra), 0) for palabra in palabras_comunes])\n","rdd_tf = rdd_frecuencias_por_texto.rightOuterJoin(rdd_vectores).map(lambda x: ((x[0][0], x[0][1]), x[1][0] if x[1][0] != None else x[1][1]))"],"metadata":{"id":"xcxtHhSxStvC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pasos: Contamos el numero de documentos que contienen a cada una de las K palabras mas comunes.\n","\n","12) Con la transformacion distinct() eliminamos los registros duplicados ya que en este caso no nos interesa saber la frecuencia en cada texto, sino que nos interesa el numero de textos que contienen a cada una de las palabras mas comunes.\n","\n","13) Con la transformacion map() buscamos obtener registros de la forma (palabra, 1) para poder contarlas.\n","\n","14) Con la accion countByKey() buscamos contar las ocurrencias de registros para cada clave."],"metadata":{"id":"jNoQHlQ5ecGy"}},{"cell_type":"code","source":["frecuencias_por_palabra = rdd_flatmap.distinct().map(lambda x: (x[0][1], x[1])).countByKey()"],"metadata":{"id":"rb3Zp4qzJ0hT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pasos: Construimos un vector para cada documento de la forma [(Texto, Palabra), Frecuencia * IDF(Palabra)]\n","\n","15) Con la transformacion map() buscamos obtener registros de la forma ((Texto, Palabra), Frecuencia * IDF(Palabra)) para asi obtener un RDD cuyos valores definen unívocamente la matriz de TF-IDF."],"metadata":{"id":"NQYkzqFTe1aK"}},{"cell_type":"code","source":["rdd_tfidf = rdd_tf.map(lambda x: ((x[0][0], x[0][1]), x[1] * math.log((cantidad_documentos+1)/frecuencias_por_palabra[x[0][1]], 10)))"],"metadata":{"id":"WCG9z0ziSqdT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Resolucion reducida**"],"metadata":{"id":"Nv2koI5AZk0t"}},{"cell_type":"code","source":["spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","sqlContext = SQLContext(sc)\n","df = sqlContext.read.csv('/content/drive/MyDrive/Organizacion de Datos/Colab Notebooks/TP1/contents_text_sample.csv', header=True, inferSchema=True, escape='\"', multiLine=True)\n","rdd = df.rdd\n","\n","rdd_map = rdd.filter(lambda x: x.text != None).map(lambda x: (x.id, re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", x.text.lower())))\\\n","             .map(lambda x: (x[0], [palabra for palabra in x[1] if ((len(palabra) > 1) and (palabra not in stopwordsParametrizable))])).cache()\n","\n","cantidad_documentos = rdd_map.count()\n","\n","palabras_comunes = rdd_map.flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).takeOrdered(featuresParametrizable, lambda x: -x[1])\n","palabras_comunes = [x[0] for x in palabras_comunes]\n","\n","rdd_flatmap = rdd_map.flatMap(lambda x: [((x[0], palabra), 1) for palabra in x[1] if palabra in palabras_comunes]).cache()\n","rdd_frecuencias_por_texto = rdd_flatmap.reduceByKey(lambda x,y: x+y)\n","rdd_vectores = rdd_map.flatMap(lambda x: [((x[0], palabra), 0) for palabra in palabras_comunes])\n","rdd_tf = rdd_frecuencias_por_texto.rightOuterJoin(rdd_vectores).map(lambda x: ((x[0][0], x[0][1]), x[1][0] if x[1][0] != None else x[1][1]))\n","\n","frecuencias_por_palabra = rdd_flatmap.distinct().map(lambda x: (x[0][1], x[1])).countByKey()\n","frecuencias_por_palabra\n","\n","rdd_tfidf = rdd_tf.map(lambda x: ((x[0][0], x[0][1]), x[1] * math.log((cantidad_documentos+1)/frecuencias_por_palabra[x[0][1]], 10)))"],"metadata":{"id":"upnHNl40Zu04"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Respuesta**"],"metadata":{"id":"9kUZQycAnBgv"}},{"cell_type":"code","source":["rdd_tfidf.take(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DKxfKdQ4c0hz","executionInfo":{"status":"ok","timestamp":1683582361170,"user_tz":180,"elapsed":32628,"user":{"displayName":"Alan Valdevenito","userId":"11872677415514201254"}},"outputId":"42fa0317-84fd-4b7d-ca95-2b8eb41248a7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[((127, 'ref'), 12.439900925118277),\n"," ((127, 'http'), 2.1151711142627394),\n"," ((127, 'categoría'), 0.5885068827959938),\n"," ((179, 'ref'), 1.036658410426523),\n"," ((179, 'http'), 1.5863783356970544)]"]},"metadata":{},"execution_count":11}]}]}